{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcb0dc06",
   "metadata": {},
   "source": [
    "### Detect unusual words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c06b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0e8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"\"\"Just forced myself to eat a slice. I'm really not hungry tho. \n",
    "           Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\"\"\"\n",
    "sent2 = \"I call you later, don't have nw. If urgnt, sms me.\"\n",
    "sent3 = \"Watching a telugu movie..wat abt u?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc90e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unusual_words(text):\n",
    "    text_vocab_set = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_set = text_vocab_set - english_vocab_set\n",
    "    return sorted(unusual_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffb7614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knows', 'lol']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0a878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nw', 'sms', 'urgnt']\n"
     ]
    }
   ],
   "source": [
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "325fe3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573eba9",
   "metadata": {},
   "source": [
    "#### Detect possible spelling mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a9f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tol', 'lola', 'pol', 'loo', 'lob', 'sol', 'lop', 'loa', 'kol', 'loll', 'lot', 'vol', 'lof', 'col', 'log', 'dol', 'gol', 'low', 'loy', 'lox', 'lou', 'lod', 'lo', 'lolo']\n"
     ]
    }
   ],
   "source": [
    "unusual_words_found = ['knows', 'lol', 'nw', 'sms', 'urgnt', 'abt']\n",
    "from nltk.metrics import edit_distance\n",
    "possible_suggestions={}\n",
    "english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "for unusual_word in unusual_words_found:\n",
    "    for word in english_vocab_set:\n",
    "        dist = edit_distance(unusual_word,word)\n",
    "        if dist<len(unusual_word)/2:\n",
    "            if unusual_word not in possible_suggestions.keys():\n",
    "                possible_suggestions[unusual_word] = [word]\n",
    "            else:\n",
    "                possible_suggestions[unusual_word].append(word)\n",
    "  \n",
    "print(possible_suggestions[\"lol\"])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736edd1",
   "metadata": {},
   "source": [
    "#### Detect names of people in the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e0f598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Mary']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def names_in_text(text):\n",
    "    names=[]\n",
    "    words_set = set(i for i in text if i.isalpha())\n",
    "    male_names = nltk.corpus.names.words('male.txt')\n",
    "    female_names = nltk.corpus.names.words('female.txt')\n",
    "    for w in words_set:\n",
    "        if w in male_names or w in female_names:\n",
    "            names.append(w)\n",
    "    return names\n",
    "sent1 = \"John and Mary go to the church every Sunday\"\n",
    "sent2 = \"No man has ever seen the dark side of the Moon\"\n",
    "print(names_in_text(word_tokenize(sent1)))\n",
    "print(names_in_text(word_tokenize(sent2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03b2b011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
      "['dog', 'domestic_dog', 'Canis_familiaris']\n",
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
      "0.2\n",
      "['goodness', 'goodness', 'commodity', 'trade_good', 'full', 'estimable', 'honorable', 'respectable', 'beneficial', 'just', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'dear', 'near', 'dependable', 'safe', 'secure', 'right', 'ripe', 'well', 'effective', 'in_effect', 'in_force', 'serious', 'sound', 'salutary', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'well', 'thoroughly', 'soundly']\n",
      "work\n",
      "deny\n",
      "abacus\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "# Get all possible meanings of the word \"dog\n",
    "print(wn.synsets(\"dog\"))\n",
    "# Get all lemma names of \"dog\"\n",
    "print(dog.lemma_names())\n",
    "#Get all hypernyms of \"dog\"\n",
    "print(wn.synset('dog.n.01').hypernyms())\n",
    "# A hypernym is the generic term where as a hyponym is a specific term\n",
    "# For the word dog, the hypernyms are 'canine' and 'domestic_animal'\n",
    "#Get all hyponyms of \"dog\"\n",
    "print(wn.synset('dog.n.01').hyponyms())\n",
    "# some of hyponyms are  \"pug\", \"puppy\", \"lap_dog\", etc..\n",
    "#Get the path similarity between to words - the method returns the shortest path in the taxonomy\n",
    "\n",
    "print(cat.path_similarity(dog)) #Returns a value between 0 and 1. The higher the number, higher the similarity in path\n",
    "# wu and palmer similarity method. \n",
    "\"\"\" Produces similarity values based on their Least Common Subsumer (most specific ancestor node) and \n",
    "Â   the maximum depth in the taxonomy\"\"\"\n",
    "cat.wup_similarity(dog)\n",
    "# Get all synonyms of the word 'good'\n",
    "synonyms = []\n",
    "for syn in wn.synsets(\"good\"):\n",
    "    for word in syn.lemmas():\n",
    "        if word.name() != \"good\":\n",
    "            synonyms.append(word.name())\n",
    "print(synonyms)\n",
    "# Get all antonyms of the word \"good\"\n",
    "antonyms = []\n",
    "for syn in wn.synsets(\"good\"):\n",
    "    for word in syn.lemmas():\n",
    "        if word.name() != \"good\" and word.antonyms():\n",
    "            antonyms.append( word.antonyms()[0].name())\n",
    "            \n",
    "print(wn.morphy(\"working\",wn.VERB))\n",
    "print(wn.morphy(\"denied\",wn.VERB))\n",
    "print(wn.morphy(\"abaci\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31655cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7575cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
